{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a5a4b1d7",
      "metadata": {
        "id": "a5a4b1d7"
      },
      "source": [
        "# Homework 7 : Linear / Nonlinear Classification\n",
        "\n",
        "Each subproblem in Problems 1 and 2 is worth 10 pts.  Problems 3 and 4 are worth 15 points each. All cells are marked with instructions to insert your code.  Please complete all cells as directed.\n",
        "\n",
        "**What to turn in**:\n",
        " -  Please print the notebook containing the answers and results into a pdf file (you can use `File - Print`). Submit this pdf file to the main homework entry in gradescope. Be sure to locate your answers for each problem when you submit, as ususal. In the worst case where you cannot print it into a pdf file somehow, you can create a Microsoft word document and then copy-paste screenshots showing your code and output parts by parts.\n",
        " -  You also need to submit this jupyter notebook file filled with your answers in the code entry in gradescope.\n",
        "\n",
        "**Description**:\n",
        "This homework will study 3-class classification in  the famous \"Iris\" dataset.  The dataset was introduced by the British statistician and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems\" as an example of linear discriminant analysis. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear classifier to distinguish the species from each other. We will do the same using classifiers that we have learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339796b0",
      "metadata": {
        "id": "339796b0"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ba020e",
      "metadata": {
        "id": "02ba020e"
      },
      "source": [
        "## Problem 1 : Load / Explore Data\n",
        "\n",
        "### (a)\n",
        "\n",
        "Because this dataset is so well-known, Scikit-Learn includes a special function for loading it, which is provided below.  Do the following in the cell below:\n",
        "  * Load the data and create a Train / Test split with 25% test data (train_test_split() function)\n",
        "  * For the above, make sure to use the provided random state so that results are repeatable\n",
        "  * Display the training inputs (you can use function display())\n",
        "\n",
        "Note: You will need the feature names later on.  It is helpful at this point to store them in a set using the DataFrame.colums property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a2bfb3b",
      "metadata": {
        "id": "4a2bfb3b"
      },
      "outputs": [],
      "source": [
        "# use this random state for train/test split\n",
        "random_state=1234\n",
        "\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac1612b8",
      "metadata": {
        "id": "ac1612b8"
      },
      "source": [
        "### (b)\n",
        "\n",
        "Now we will explore our feature distributions.  In the cell below, use the Pandas DataFrame plot feature to plot the density of each feature in the training data.\n",
        "\n",
        "[ Documentation - Pandas - DataFrame.plot ](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fafc8bc0",
      "metadata": {
        "id": "fafc8bc0"
      },
      "outputs": [],
      "source": [
        "# Insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf93effa",
      "metadata": {
        "id": "cf93effa"
      },
      "source": [
        "### (c)\n",
        "\n",
        "Sometimes it is better to look at distributions of each feature plotted together.  In the cell below produce a boxplot (use plt.boxplot()) of each feature in the training data.  **Make sure to rotate X-tick labels 45-degrees so they are readable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146f6b51",
      "metadata": {
        "id": "146f6b51"
      },
      "outputs": [],
      "source": [
        "# Insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9445cee6",
      "metadata": {
        "id": "9445cee6"
      },
      "source": [
        "### (d)\n",
        "\n",
        "Now let's see how well we can separate classes from each pair of features.  In the cell below produce a scatterplot of **every pair of features** in the training data.  There will be 6 scatterplots in all.  Make sure to follow these instructions:\n",
        "  * Color each marker red, green, or blue depending on the true class label\n",
        "  * Use numpy.corrcoef to compute the correlation coefficient of each feature\n",
        "  * Title each plot with the correlation coefficient\n",
        "  * Label each axis using the corresponding feature name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198a9247",
      "metadata": {
        "id": "198a9247"
      },
      "outputs": [],
      "source": [
        "# Insert code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b5b468",
      "metadata": {
        "id": "98b5b468"
      },
      "source": [
        "## Problem 2 : Train a logistic regression classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42522e87",
      "metadata": {
        "id": "42522e87"
      },
      "source": [
        "### (a)\n",
        "\n",
        "Now we will look at finding the best feature out of all the features. To do this, you will preform Cross Validation of Logustic Regression. We will break this into subproblems to walk through it. In the cell do the following:\n",
        "* Using LogisticRegressionCV perform 5-fold cross validation to train on each feature\n",
        "* For each run use Matplotlib errorbar() to plot the average +/- standard deviation of error versus regularization coefficient (the property LogisticRegressionCV.Cs_) -- there should be 4 plots in total\n",
        "* Set plot X-label with the feature name, and Y-label \"Accuracy\"\n",
        "* Title each plot with the maximum achieved accuracy score\n",
        "* Report the best accuracy from cross-validation\n",
        "* Finally, report the best performing feature and save it for later\n",
        "\n",
        "Make sure to set the following properties in LogisticRegressionCV:\n",
        "* cv=5\n",
        "* max_iter=1e4\n",
        "* random_state=0\n",
        "* multi_class='multinomial'\n",
        "* Cs=10\n",
        "\n",
        "[Documentation - Scikit-Learn - LogisticRegressionCV](https://scikit-learn.org/0.16/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8996b76",
      "metadata": {
        "id": "a8996b76"
      },
      "outputs": [],
      "source": [
        "# Insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a49114",
      "metadata": {
        "id": "96a49114"
      },
      "source": [
        "### (b)\n",
        "\n",
        "Now lets look at all pairs of features.  The cell below provides a function plotLogreg2feat() to visualize the learned classifier for a pair of features.  This function will draw the decision boundaries for each of the three classes, which will give us a better picture of what's going on.  In the cell below that do the following:\n",
        "* Loop over every pair of features (there are 6 pairs total)\n",
        "* Using LogisticRegressionCV perform 5-fold cross validation to train a classifier on the pair of features\n",
        "* Make sure to use **the same cross validation options as the previous experiment**\n",
        "* Using plotLogreg2feat plot the learned classifier\n",
        "* Title each plot with the maximum average accuracy from cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc0a548",
      "metadata": {
        "id": "8fc0a548"
      },
      "outputs": [],
      "source": [
        "def plotLogreg2feat(X, featname_1, featname_2, model):\n",
        "    '''\n",
        "    INPUTS:\n",
        "      X - Input DataFrame (assumes Nx2 for N data points and 2 features)\n",
        "      featname_1, featname_2 - String containing feature names\n",
        "      model - Fitted LogisticRegressionCV model\n",
        "\n",
        "    OUTPUTS:\n",
        "      ax - Returns figure axis object\n",
        "    '''\n",
        "\n",
        "    # make grid\n",
        "    x_min, x_max = X[featname_1].min() - 0.5*X[featname_1].std(), X[featname_1].max() + 0.5*X[featname_1].std()\n",
        "    y_min, y_max = X[featname_2].min() - 0.5*X[featname_2].std(), X[featname_2].max() + 0.5*X[featname_2].std()\n",
        "    h = 0.02  # step size in the mesh\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    #plt.figure(1, figsize=(4, 3))\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    # Plot also the training points\n",
        "    #plt.scatter(X_train[features[i]][ Y_train == 0 ], X_train[features[j]][ Y_train == 0 ], c='r')\n",
        "    #plt.scatter(X_train[features[i]][ Y_train == 1 ], X_train[features[j]][ Y_train == 1 ], c='g')\n",
        "    #plt.scatter(X_train[features[i]][ Y_train == 2 ], X_train[features[j]][ Y_train == 2 ], c='b')\n",
        "\n",
        "    ax.scatter(X[featname_1][ y == 0 ], X[featname_2][ y == 0 ], c='r')\n",
        "    ax.scatter(X[featname_1][ y == 1 ], X[featname_2][ y == 1 ], c='g')\n",
        "    ax.scatter(X[featname_1][ y == 2 ], X[featname_2][ y == 2 ], c='b')\n",
        "\n",
        "    ax.set_xlabel(featname_1)\n",
        "    ax.set_ylabel(featname_2)\n",
        "\n",
        "\n",
        "    #plt.xlim(xx.min(), xx.max())\n",
        "    #plt.ylim(yy.min(), yy.max())\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a30859",
      "metadata": {
        "id": "08a30859"
      },
      "outputs": [],
      "source": [
        "# Insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de803f23",
      "metadata": {
        "id": "de803f23"
      },
      "source": [
        "### (c)\n",
        "\n",
        "Surprisingly, adding pairs of features doesn't seem to improve things.  Let's try training on all features.  In the cell below:\n",
        "* Perform 5-fold cross validation (using all the same parameters as before) to train a logistic regression classifier on all features\n",
        "* Report the maximum of the average scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fef03b9c",
      "metadata": {
        "id": "fef03b9c"
      },
      "outputs": [],
      "source": [
        "# Insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2752c86a",
      "metadata": {
        "id": "2752c86a"
      },
      "source": [
        "If your results are the same as mine, the maximum score over all features is the same as over the best single feature."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d20c0671",
      "metadata": {
        "id": "d20c0671"
      },
      "source": [
        "## Problem 3 : Support Vector Machine\n",
        "We have trained several logistic regression classifiers, all of which achieve an a cross-validation accuracy well into the 90% range.  In an effort to see if we can do better, let's train one lass classifier--a support vector machine.  For this classifier we will introduce a nonlinear tranformation using the Radial Basis kernel function.  In the cell be low do the following:\n",
        "* Using Numpy.logspace create a logarithmically spaced set of 100 regularization coefficient in the range (1e-4, 1e4)\n",
        "* For each coefficient define a support vector classifier with kernel='rbf' and set the regularization coefficient (C=coefficient)\n",
        "* Perform 5-fold cross validation (e.g. using cross_val_score)\n",
        "* Plot the average accuracy versus regularization coefficient and report the maximum accuracy and best coefficient\n",
        "* Make sure to set the plot X-scale to 'log' and label axes and title\n",
        "\n",
        "[ Documentation - Scikit-Learn - svm.SVC ](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC.score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cfc5b8",
      "metadata": {
        "id": "93cfc5b8"
      },
      "outputs": [],
      "source": [
        "# Insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f17b8077",
      "metadata": {
        "id": "f17b8077"
      },
      "source": [
        "My results show slightly higher accuracy under the SVM classifier.  However, cross_val_score does not use the same cross validation splits as the built-in cross validation of LogisticRegressionCV (which randomizes splits).  So we shall see..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e81c5c2",
      "metadata": {
        "id": "1e81c5c2"
      },
      "source": [
        "## Problem 4 : Evaluate on test\n",
        "Now we will evaluate all classifiers on the test data.  Take the best regression classifier and the best SVM classifier (with previously chosen parameters), train them, and evaluate accordingly.  For each classifier report:\n",
        "  * Test accuracy\n",
        "  * Confusion matrix\n",
        "  * Results of classification_report\n",
        "  \n",
        "We have only left a single cell below.  Feel free to insert additional cells and arrange output as you see fit.  Make sure it is readable.  Feel free to explore any additional visualizations or metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66297f41",
      "metadata": {
        "id": "66297f41"
      },
      "outputs": [],
      "source": [
        "# Insert code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}